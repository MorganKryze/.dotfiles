#!/run/current-system/sw/bin/bash

# Opens a VS Code window in the .dotfiles project
function config() {
	cd ~/.dotfiles
	opn
}

function nix-update() {
	clear
	cat <<"EOF"
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣴⡾⢷⡄⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⣀⣠⣤⣴⠶⠶⠾⠋⠀⠘⠿⠶⣦⣄⡀⠀⠀⣀⣤⣄⡀⠀⠀⠀⠀⠀⠀⠀⠀
⠀⠀⣶⣤⣄⣀⣀⣤⡶⠟⠋⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠉⠻⢶⣼⣿⡭⢍⢿⡆⠀⠀⠀⠀⠀⠀⠀
⠀⠀⣿⡄⠉⠙⠛⠁⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⢀⣾⠇⠈⢿⠀⢸⡇⠀⠀⠀⠀⠀⠀⠀
⠀⠀⢸⣇⠀⠀⠀⢀⣤⣤⣤⡀⠀⠀⠀⠀⠀⠀⠀⠀⢀⡀⠺⠗⠂⠛⠀⠀⠀⠀⣸⡇⠀⠀⠀⠀⠀⠀⠀
⠀⠀⢈⣿⢦⠀⣴⢿⣯⢷⠸⣧⢀⣶⡄⠀⠘⢷⠟⠛⠛⠁⠀⠀⢀⣀⣠⣤⣤⣶⡟⢀⣀⡀⠀⠀⠀⠀⠀
⠀⢀⡾⠃⠀⠒⠏⠨⣿⠶⢓⣿⠀⠉⠁⠀⣀⣀⣤⣴⡶⠶⣾⣛⣻⣽⣿⣶⡾⢾⣟⠛⠛⢿⡀⠀⠀⠀⠀
⢀⣿⠁⠀⠀⠀⢀⣀⣒⣴⣾⣷⣶⣶⣿⣿⣯⣿⠿⠾⠛⠛⣿⡋⠉⠀⢿⡄⠀⠀⢻⣆⠀⠈⢿⣦⡀⠀⠀
⠹⣿⡴⠶⠟⠛⠛⠋⢉⣿⡿⠿⠛⠻⣯⠁⠀⠻⣦⡀⠀⠀⠙⣷⡀⠀⠀⢻⣆⣀⣤⣿⠶⢶⣿⣿⣿⣦⡀
⠀⠀⠀⠀⠀⠀⠀⠀⢾⡏⢻⣆⠀⠀⠙⢷⣄⣀⣨⣷⣤⣤⡶⢾⣿⣻⣿⣿⣿⣿⡿⠾⠛⠛⠛⠉⠀⣿⡇
⠀⠀⠀⠀⠀⠀⠀⠀⠈⢷⣄⠹⣷⠟⣛⣻⣯⣭⣿⠾⠿⠿⠛⠛⠋⠉⠁⠀⢀⣀⣠⣤⣤⣶⠶⠶⠿⠻⠃
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠻⣆⣿⡟⠉⠉⠀⠀⣀⢀⣀⣀⣀⣤⣤⡴⠶⠟⠛⠉⠉⠁⠀⠀⠀⠀⠀⠀⠀
⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠙⠿⠟⠛⠛⠛⠛⠛⠛⠛⠉⠉⠉⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀⠀
EOF
	description "nix-update" "updates the nix flake and rebuilds the system taking the new configuration into account."

	cd $HOME/.dotfiles
	info "Updating nix flake..."
	nix flake update --impure || error "Failed to update nix flake."
	darwin-rebuild switch --flake $HOME/.dotfiles#concord --impure || error "Failed to rebuild the system."

	info "Cleaning up artifacts..."
	brew cleanup -s || error "Failed to clean up brew artifacts."
	cd -

	info "Saving the timestamp into a file..."
	date +%s >$HOME/.last-nix-update || error "Failed to save the timestamp."

	success "System has been updated."
}

function nix-update-check() {
	if [ ! -f $HOME/.last-nix-update ]; then
		warning "Last update unknown. Please run :'nix-update' to update the system."
		return 1
	fi

	local last_update=$(cat $HOME/.last-nix-update)
	local current_time=$(date +%s)
	local diff=$((current_time - last_update))

	if [ $diff -gt 604800 ]; then
		warning "Nix flake outdated. Please run :'nix-update' to update the system."
	fi
}

# List all processes using a disk
function processes-using-disk() {
	description "processes-using-disk" "lists all processes using a disk."

	local disk_name="$1"
	sudo lsof /Volumes/$disk_name || error "Failed to list processes using disk." || return 1

	success "Processes using the disk have been listed."
}

# Kill all processes using a disk
function kill-processes-using-disk() {
	description "kill-processes-using-disk" "kills all processes using a disk."

	local disk_name="$1"
	sudo lsof /Volumes/$disk_name | awk 'NR > 1 {print $2}' | sudo xargs kill || error "Failed to kill processes using disk." || return 1

	success "Processes using the disk have been killed."
}

# Disable spotlight indexing for a disk
function disable-spotlight-indexing-for-disk() {
	description "disable-spotlight-indexing-for-disk" "disables spotlight indexing for a disk."

	local disk_name="$1"
	sudo mdutil -d "/Volumes/$disk_name" || error "Failed to disable spotlight indexing for disk." || return 1
	sudo mdutil -X "/Volumes/$disk_name" || error "Failed to disable spotlight indexing for disk." || return 1

	success "Spotlight indexing has been disabled for the disk."
}

# Create a random hash
function rnd-hash() {
	description "rnd-hash" "creates a random hash."

	txt $(openssl rand -hex 6) || error "Failed to create random hash."
}

# Hash a string
function hash() {
	local string="$1"
	txt $(echo -n "$string" | shasum -a 256 | awk '{print $1}')
}

# Secured hash a string with a salt
function shash() {
	local string="$1"
	local salt=$(head -c 16 /dev/urandom | base64 | tr -d '=+/' | cut -c1-16)
	local combined="${string}${salt}"
	local hash=$(echo -n "$combined" | shasum -a 256 | awk '{print $1}')
	txt "${hash}${salt}"
}

# Create a new SSH key pair for GitHub
function create-git-ssh-connections() {
	description "create-github-ssh-connection" "sets up a new SSH key pair for GitHub."

	source ~/.dotfiles/.env

	info "Generating SSH key..."
	ssh-keygen -t ed25519 -C "$GITHUB_EMAIL" -f ~/.ssh/git -p "" || error "Failed to generate SSH key." || return 1

	info "Starting the ssh-agent..."
	eval "$(ssh-agent -s)" || error "Failed to start the ssh-agent." || return 1

	info "Adding the SSH key to the ssh config for GitHub and GitLab..."
	echo "Host github.com
  PreferredAuthentications publickey
  IdentityFile ~/.ssh/git" >~/.ssh/config || error "Failed to write SSH config." || return 1

	echo "Host gitlab.com
  PreferredAuthentications publickey
  IdentityFile ~/.ssh/git" >>~/.ssh/config || error "Failed to write SSH config." || return 1

	info "Adding the SSH key to the ssh-agent..."
	ssh-add ~/.ssh/git || error "Failed to add SSH key to the ssh-agent." || return 1

	info "Copying the SSH key to the clipboard..."
	pbcopy <~/.ssh/git.pub || error "Failed to copy SSH key to clipboard." || return 1

	info "The SSH key has been copied to the clipboard. Please add it to your GitHub account."
	open "https://github.com/settings/keys" || error "Failed to open GitHub keys page." || return 1

	acknowledge "Insert your key in the GitHub account before continuing..."

	open "https://gitlab.com/-/user_settings/ssh_keys" || error "Failed to open GitLab keys page." || return 1

	acknowledge "Insert your key in the GitLab account before continuing..."

	success "SSH key has been set up successfully."
}

# Delete all .DS_Store files in a folder and its subdirectories
function flush() {
	description "flush" "deletes all .DS_Store files in a folder and its subdirectories."

	local folder="$1"

	if [ -z "$folder" ]; then
		error "Usage: delete_ds_store <folder>" || return 1
	fi

	if [ ! -d "$folder" ]; then
		error "Error: $folder is not a directory" || return 1
	fi

	find "$folder" -name ".DS_Store" -type f -exec rm -f {} \;
	success "All .DS_Store files in $folder and its subdirectories have been deleted."
}

# Open the superfile app
function spf() {
	export SPF_LAST_DIR="$HOME/Library/Application Support/superfile/lastdir"

	superfile "$@"

	[ ! -f "$SPF_LAST_DIR" ] || {
		. "$SPF_LAST_DIR"
		rm -f -- "$SPF_LAST_DIR" >/dev/null
	}
}

function backup-fab() {
	description "backup-fab" "creates a backup of the fab server data with improved reliability and efficiency."

	local REMOTE_USER="debian"
	local REMOTE_HOST="devinci-fablab.fr"
	local REMOTE_DIR="/backups"
	local LOCAL_DIR="/Users/morgan/Documents/00-local/00-storage/Backups_fablab"
	local TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
	local BACKUP_FILE="backup_dvfl_server_${TIMESTAMP}.tar.gz"
	local LOCKFILE="/tmp/backup_fab.lock"

	# Check if backup is already running
	if [ -f "$LOCKFILE" ]; then
		error "Backup is already running. Lock file exists: $LOCKFILE"
		return 1
	fi

	# Create lock file
	echo $$ >"$LOCKFILE"
	trap "rm -f $LOCKFILE" EXIT

	# Ensure local directory exists
	mkdir -p "$LOCAL_DIR" || {
		error "Failed to create local backup directory."
		return 1
	}

	info "Pinning remote host..."
	ping devinci-fablab.fr -c 1 >/dev/null || {
		error "Remote host is unreachable. Please check your network connection."
		rm -f "$LOCKFILE"
		return 1
	}

	# Check remote directory exists and has content
	info "Checking remote directory..."
	ssh -t ${REMOTE_USER}@${REMOTE_HOST} "sudo test -d ${REMOTE_DIR} && [ \$(sudo ls -A ${REMOTE_DIR} | wc -l) -gt 0 ]" || {
		error "Remote directory is empty or doesn't exist."
		return 1
	}

	# Check available space on remote server
	info "Checking available space on remote server..."
	local remote_space=$(ssh ${REMOTE_USER}@${REMOTE_HOST} "df /tmp | tail -1 | awk '{print \$4}'")
	local backup_size=$(ssh -t ${REMOTE_USER}@${REMOTE_HOST} "sudo du -s ${REMOTE_DIR} | awk '{print \$1}'")

	if [ "$remote_space" -lt "$backup_size" ]; then
		error "Insufficient space on remote server for backup."
		return 1
	fi

	info "Creating incremental backup with compression..."
	# Use pigz for faster compression if available, exclude unnecessary files
	ssh -t ${REMOTE_USER}@${REMOTE_HOST} "
        sudo tar --exclude='*.log' --exclude='*.tmp' --exclude='cache/*' \
        -I 'pigz -p 4' -cf /tmp/${BACKUP_FILE} -C ${REMOTE_DIR} . 2>/dev/null || \
        sudo tar -czf /tmp/${BACKUP_FILE} -C ${REMOTE_DIR} .
    " || {
		error "Failed to create backup on remote server."
		return 1
	}

	info "Verifying backup integrity..."
	ssh ${REMOTE_USER}@${REMOTE_HOST} "test -f /tmp/${BACKUP_FILE} && tar -tzf /tmp/${BACKUP_FILE} >/dev/null" || {
		error "Backup file is corrupted or missing."
		return 1
	}

	info "Downloading backup with progress and resume capability..."
	# Use rsync for better transfer reliability
	rsync -avz --progress --partial \
		${REMOTE_USER}@${REMOTE_HOST}:/tmp/${BACKUP_FILE} \
		${LOCAL_DIR}/ || {
		error "Failed to download backup from remote server."
		return 1
	}

	info "Verifying downloaded backup..."
	tar -tzf "${LOCAL_DIR}/${BACKUP_FILE}" >/dev/null || {
		error "Downloaded backup is corrupted."
		return 1
	}

	info "Cleaning up remote temporary file..."
	ssh -t ${REMOTE_USER}@${REMOTE_HOST} "sudo rm -f /tmp/${BACKUP_FILE}" || {
		warning "Failed to remove temporary backup file from remote server."
	}

	local backup_size_mb=$(du -m "${LOCAL_DIR}/${BACKUP_FILE}" | cut -f1)
	success "Backup completed successfully: ${LOCAL_DIR}/${BACKUP_FILE} (${backup_size_mb}MB)"
}
